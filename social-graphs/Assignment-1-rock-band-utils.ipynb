{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb907ef7",
   "metadata": {},
   "source": [
    "The following code below is all of the Network Rock Band exercise implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3b5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "import networkx as nx\n",
    "import os\n",
    "from concurrent import futures\n",
    "from threading import Lock\n",
    "from urllib.parse import quote\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7048b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all band names from the list of mainstream rock performers\n",
    "band_pattern = r\"\\[{2}([^#\\[\\]<>{}|_]*)[|#]?[^#\\[\\]<>{}|_]*\\]{2}\"\n",
    "\n",
    "baseurl = \"https://en.wikipedia.org/w/api.php?\"\n",
    "action = \"action=query\"\n",
    "content = \"prop=revisions&rvprop=content\"\n",
    "dataformat = \"format=json\"\n",
    "headers = {\"User-Agent\": \"MyApp/1.0 (your_email@example.com)\"}\n",
    "\n",
    "\n",
    "def fetch_wiki_rock_bands_content() -> list[str]:\n",
    "    title = \"titles=List_of_mainstream_rock_performers\"\n",
    "    query = \"{}{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat)\n",
    "    wikiresponse = requests.get(query, headers=headers)\n",
    "    wikiJSON = json.loads(wikiresponse.content.decode())\n",
    "    return next(iter(wikiJSON[\"query\"][\"pages\"].values()))[\"revisions\"][0][\"*\"]\n",
    "\n",
    "\n",
    "def generate_band_query_url(band_name: str):\n",
    "    title = f\"titles={quote(band_name, safe='_')}\"\n",
    "    return \"{}{}&{}&{}&{}\".format(baseurl, action, content, title, dataformat)\n",
    "\n",
    "\n",
    "ignore_matches = [\n",
    "    \"AllMusic\",\n",
    "    \"rock music\",\n",
    "    \"Category:\",\n",
    "    \"Template:\",\n",
    "    \"Help:\",\n",
    "    \"File:\",\n",
    "    \"Special:\",\n",
    "    \"Wikipedia:\",\n",
    "    \"Portal:\",\n",
    "    \"Draft:\",\n",
    "    \"Talk:\",\n",
    "]\n",
    "bands: list[str] = re.findall(band_pattern, fetch_wiki_rock_bands_content())\n",
    "\n",
    "bands = [b for b in bands if not any(b.startswith(prefix) for prefix in ignore_matches)]\n",
    "bands_query_urls = [generate_band_query_url(band) for band in bands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d22c7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "488"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df963797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Wikipedia pages in parallel\n",
    "if not os.path.exists(\"wiki_pages\"):\n",
    "    os.makedirs(\"wiki_pages\")\n",
    "\n",
    "\n",
    "def wiki_page_file_exists(page_name: str):\n",
    "    filename = f\"wiki_pages/{page_name.replace(' ', '_').replace('/', '_')}.txt\"\n",
    "    return os.path.exists(filename) and os.path.getsize(filename) > 0\n",
    "\n",
    "\n",
    "def download_wiki_page(band_query: str, band_name: str):\n",
    "    # Check if file already exists and is non-empty\n",
    "    if wiki_page_file_exists(band_name):\n",
    "        with open(\n",
    "            f\"wiki_pages/{band_name.replace(' ', '_').replace('/', '_')}.txt\",\n",
    "            \"r\",\n",
    "            encoding=\"utf-8\",\n",
    "        ) as f:\n",
    "            content = f.read()\n",
    "        return {\"page_name\": band_name, \"data\": content}\n",
    "    wikiresponse = requests.get(band_query, headers=headers)\n",
    "    wikiJSON = json.loads(wikiresponse.content.decode())\n",
    "    page = next(iter(wikiJSON[\"query\"][\"pages\"].values()))\n",
    "    if \"revisions\" not in page:\n",
    "        return {\"page_name\": band_name, \"data\": None}\n",
    "    content = page[\"revisions\"][0][\"*\"]\n",
    "    return {\"page_name\": band_name, \"data\": content}\n",
    "\n",
    "\n",
    "def save_page_content(page_name: str, content: str):\n",
    "    \"\"\"Save page content to a file\"\"\"\n",
    "    filename = f\"wiki_pages/{page_name.replace(' ', '_').replace('/', '_')}.txt\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "    return filename\n",
    "\n",
    "\n",
    "# band - wiki-content mapping\n",
    "downloaded_pages = {}\n",
    "max_workers = 100\n",
    "download_lock = Lock()\n",
    "\n",
    "with futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    future_to_page = {\n",
    "        executor.submit(\n",
    "            download_wiki_page,\n",
    "            band_query,\n",
    "            bands[bands_query_urls.index(band_query)],\n",
    "        ): bands[bands_query_urls.index(band_query)]\n",
    "        for band_query in bands_query_urls\n",
    "    }\n",
    "\n",
    "    for i, future in enumerate(futures.as_completed(future_to_page), 1):\n",
    "        band_name = future_to_page[future]\n",
    "        result = future.result()\n",
    "        if result[\"data\"] is not None:\n",
    "            downloaded_pages[result[\"page_name\"]] = result[\"data\"]\n",
    "            # Only save if file does not already exist and is non-empty\n",
    "            if not wiki_page_file_exists(result[\"page_name\"]):\n",
    "                save_page_content(result[\"page_name\"], result[\"data\"])\n",
    "\n",
    "print(f\"{len(downloaded_pages)} out of {len(bands)} band wiki pages downloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf28bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_related_bands(band_name: str):\n",
    "    bands_text = downloaded_pages[band_name]\n",
    "    related_bands = re.findall(band_pattern, bands_text)\n",
    "    return related_bands\n",
    "\n",
    "\n",
    "G_bands = nx.DiGraph()\n",
    "for band in bands:\n",
    "    G_bands.add_node(band)\n",
    "    related_bands = find_related_bands(band)\n",
    "    for related_band in related_bands:\n",
    "        if related_band in bands and related_band != band:\n",
    "            G_bands.add_edge(band, related_band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519052ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_adjlist(\n",
    "    G_bands,\n",
    "    \"data/rock_band_network.gz\",\n",
    "    comments=\"#\",\n",
    "    delimiter=\"|\",\n",
    ")\n",
    "\n",
    "G_read = nx.read_adjlist(\n",
    "    \"data/rock_band_network.gz\",\n",
    "    comments=\"#\",\n",
    "    delimiter=\"|\",\n",
    "    encoding=\"utf-8\",\n",
    "    create_using=nx.DiGraph(),\n",
    ")\n",
    "print(G_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31af67e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "node_link = nx.readwrite.node_link_data(G_bands)\n",
    "out_path = \"data/rock_graph_node_link.json.gz\"\n",
    "with gzip.open(out_path, \"wt\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(node_link, fh, separators=(\",\", \":\"), ensure_ascii=False)\n",
    "print(\"wrote\", out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
